{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from transformers import pipeline, BertModel, BertTokenizerFast\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psak = r'פ\\s*ס\\s*ק\\s*-*\\s*ד\\s*י\\s*ן'\n",
    "psak = r'פ\\s*ס\\s*ק\\s*-*\\s*ד\\s*י\\s*ן\\s*\\n'\n",
    "def extract_middle_content(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        pattern = re.compile(fr'{psak}([\\s\\S]*)[\\s-]*ניתן[\\s-]*היום', re.DOTALL)\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            extracted_text = match.group(1).strip()\n",
    "            extracted_percentage = len(extracted_text) / len(text) * 100\n",
    "            if extracted_percentage == 0:\n",
    "                return text\n",
    "            return extracted_text\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # If the previous pattern fails, extract from the last 'פסק-דין' to the end\n",
    "        pattern = re.compile(fr'.*{psak}([\\s\\S]*)', re.DOTALL)\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            extracted_text = match.group(1).strip()\n",
    "            extracted_percentage = len(extracted_text) / len(text) * 100\n",
    "            if extracted_percentage == 0:\n",
    "                return text\n",
    "            return extracted_text\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        # If there's no 'פסק-דין', extract from the beginning to 'ניתן היום'\n",
    "        pattern = re.compile(r'^([\\s\\S]*)(ניתן[\\s-]*היום)', re.DOTALL)\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            extracted_text = match.group(1).strip()\n",
    "            extracted_percentage = len(extracted_text) / len(text) * 100\n",
    "            if extracted_percentage == 0:\n",
    "                return text\n",
    "            return extracted_text\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "    # If there's no 'פסק-דין', extract from the beginning\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/mnt/local/mikehash/Data/Nevo/NevoVerdicts\"\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data.append(file.read())\n",
    "        except:\n",
    "            print(file_path)\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "df['extracted_content'] = df['text'].apply(lambda x: extract_middle_content(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['extracted_content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embedding = pipeline(\"feature-extraction\", model=\"onlplab/alephbert-base\", device='cuda:0')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base', device='cuda:0')\n",
    "model = BertModel.from_pretrained('onlplab/alephbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('feature-extraction', model='onlplab/alephbert-base', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(data[0], truncation=True, padding=True)[0][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/Legal-heBERT\", max_length=512, device='cuda:0', truncation=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"avichr/Legal-heBERT\")\n",
    "# tokenizer.to('cuda:0')\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = 'avichr/Legal-heBERT_ft' # for the fine-tuned HeBERT model \\\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline('feature-extraction', model=model_name, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_vs_token_counts(documents, tokenizer):\n",
    "    \n",
    "    max_word_limit = 3000  # Set your desired maximum word limit\n",
    "    \n",
    "    for document in documents:\n",
    "        word_counts = []\n",
    "        token_counts = []\n",
    "        word_limit_range = range(1000, max_word_limit, 300)  # Change step size as needed\n",
    "        \n",
    "        for word_limit in word_limit_range:\n",
    "#             # Get a subset of the document up to the specified word limit\n",
    "#             subset_document = ' '.join(document.split()[:word_limit])\n",
    "            \n",
    "#             # Calculate word count\n",
    "#             words = subset_document.split()\n",
    "#             word_counts.append(len(words))\n",
    "            # Get a subset of the document up to the specified word limit\n",
    "            subset_document = document[:word_limit]\n",
    "            \n",
    "            # Calculate word count\n",
    "            word_counts.append(word_limit)\n",
    "            \n",
    "            # Tokenize the document subset\n",
    "            tokens = tokenizer(subset_document, add_special_tokens=True)['input_ids']\n",
    "            token_counts.append(len(tokens))\n",
    "        \n",
    "        # Create the scatter plot for this document\n",
    "        plt.scatter(word_counts, token_counts, label=f'Document {documents.index(document)+1}')\n",
    "    \n",
    "    # Draw a line at 512 tokens\n",
    "    plt.axhline(y=512, color='red', linestyle='--', label='512 Tokens Limit')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Number of Tokens')\n",
    "    plt.title('Word Count vs Token Count')\n",
    "    \n",
    "    # Add legend\n",
    "#     plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    \"This is document 1 with some text.\",\n",
    "    \"Another document, a bit longer this time.\",\n",
    "    \"A very long document with many words to test the token limit.\",\n",
    "]\n",
    "\n",
    "plot_word_vs_token_counts(data[20:40], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers.pipelines import Pipeline\n",
    "\n",
    "from bertopic.backend import BaseEmbedder\n",
    "\n",
    "\n",
    "class CustomEmbedder(BaseEmbedder):\n",
    "    def __init__(self, embedding_model: Pipeline):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(embedding_model, Pipeline):\n",
    "            self.embedding_model = embedding_model\n",
    "        else:\n",
    "            raise ValueError(\"Please select a correct transformers pipeline. For example: \"\n",
    "                             \"pipeline('feature-extraction', model='distilbert-base-cased', device=0)\")\n",
    "\n",
    "    def embed(self,\n",
    "              documents: List[str],\n",
    "              max_tokens=512,\n",
    "              verbose: bool = False) -> np.ndarray:\n",
    "        \"\"\" Embed a list of n documents/words into an n-dimensional\n",
    "        matrix of embeddings\n",
    "\n",
    "        Arguments:\n",
    "            documents: A list of documents or words to be embedded\n",
    "            maximum_tokens: Maximum number of tokens per chunk\n",
    "            verbose: Controls the verbosity of the process\n",
    "\n",
    "        Returns:\n",
    "            Document/words embeddings with shape (n, m) with `n` documents/words\n",
    "            that each have an embeddings size of `m`\n",
    "        \"\"\"\n",
    "        max_tokens=512\n",
    "        embeddings = []\n",
    "        for document in tqdm(documents, total=len(documents), disable=not verbose):\n",
    "            chunks = self._split_document(document, max_tokens)\n",
    "            chunk_embeddings = [self._embed(chunk) for chunk in chunks]\n",
    "            embeddings.append(np.mean(chunk_embeddings, axis=0))\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def _split_document(self, document: str, max_tokens: int) -> List[str]:\n",
    "        tokens = self.embedding_model.tokenizer(document, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        chunks = [tokens[:, i:i + max_tokens-7] for i in range(0, tokens.size(1), max_tokens-7)]\n",
    "        return [self.embedding_model.tokenizer.decode(chunk[0].tolist(), skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "\n",
    "    def _embed(self, chunk) -> np.ndarray:\n",
    "        \"\"\" Mean pooling\n",
    "\n",
    "        Arguments:\n",
    "            chunk: The document chunk for which to extract the attention mask\n",
    "        \"\"\"\n",
    "        features = self.embedding_model(chunk, truncation=True, padding=True)\n",
    "        token_embeddings = np.array(features)\n",
    "        attention_mask = self.embedding_model.tokenizer(chunk, truncation=True, padding=True, return_tensors=\"np\")[\"attention_mask\"]\n",
    "        input_mask_expanded = np.broadcast_to(np.expand_dims(attention_mask, -1), token_embeddings.shape)\n",
    "        sum_embeddings = np.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = np.clip(input_mask_expanded.sum(1), a_min=1e-9, a_max=input_mask_expanded.sum(1).max())\n",
    "        embedding = normalize(sum_embeddings / sum_mask)[0]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\" Dataset to pass to `transformers.pipelines.pipeline` \"\"\"\n",
    "    def __init__(self, docs):\n",
    "        self.docs = docs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.docs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('feature-extraction', model=model_name, device='cuda:0')\n",
    "custom_embedder = CustomEmbedder(embedding_model=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_truncated = [d[:1400] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open('heb_stopwords.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "vectorizer_model = CountVectorizer(stop_words=stop_words)\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_model = BERTopic(embedding_model=custom_embedder, ctfidf_model=ctfidf_model, vectorizer_model=vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(data)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, _ = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-env",
   "language": "python",
   "name": "python-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
